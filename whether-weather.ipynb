{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2854929,"sourceType":"datasetVersion","datasetId":1715246}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import datasets, transforms\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, WeightedRandomSampler, Subset\nfrom collections import Counter\nimport torchmetrics\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:15.121871Z","iopub.execute_input":"2025-08-03T09:35:15.122202Z","iopub.status.idle":"2025-08-03T09:35:29.129978Z","shell.execute_reply.started":"2025-08-03T09:35:15.122177Z","shell.execute_reply":"2025-08-03T09:35:29.128973Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset_path = os.path.join(\"/kaggle/input\", \"weather-dataset\", \"dataset\")\ndataset_classes = os.listdir(dataset_path)\nfor dataset_class in dataset_classes:\n    num_files = len([x for x in os.listdir(os.path.join(dataset_path, dataset_class))])\n    print(f\"Weather type: {dataset_class}, Num_images: {num_files}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:29.131638Z","iopub.execute_input":"2025-08-03T09:35:29.132106Z","iopub.status.idle":"2025-08-03T09:35:29.267352Z","shell.execute_reply.started":"2025-08-03T09:35:29.132055Z","shell.execute_reply":"2025-08-03T09:35:29.266327Z"}},"outputs":[{"name":"stdout","text":"Weather type: hail, Num_images: 591\nWeather type: rainbow, Num_images: 232\nWeather type: frost, Num_images: 475\nWeather type: rime, Num_images: 1160\nWeather type: fogsmog, Num_images: 851\nWeather type: snow, Num_images: 621\nWeather type: rain, Num_images: 526\nWeather type: glaze, Num_images: 639\nWeather type: lightning, Num_images: 377\nWeather type: sandstorm, Num_images: 692\nWeather type: dew, Num_images: 698\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),        \n    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(                           \n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\nfull_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\nprint(\"Datasets constructed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:29.268278Z","iopub.execute_input":"2025-08-03T09:35:29.268630Z","iopub.status.idle":"2025-08-03T09:35:38.331029Z","shell.execute_reply.started":"2025-08-03T09:35:29.268606Z","shell.execute_reply":"2025-08-03T09:35:38.329819Z"}},"outputs":[{"name":"stdout","text":"Datasets constructed\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(full_dataset.class_to_idx)\nidx_to_class = {val: key for key, val in full_dataset.class_to_idx.items()}\nprint(idx_to_class)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:38.332263Z","iopub.execute_input":"2025-08-03T09:35:38.332654Z","iopub.status.idle":"2025-08-03T09:35:38.339438Z","shell.execute_reply.started":"2025-08-03T09:35:38.332624Z","shell.execute_reply":"2025-08-03T09:35:38.338413Z"}},"outputs":[{"name":"stdout","text":"{'dew': 0, 'fogsmog': 1, 'frost': 2, 'glaze': 3, 'hail': 4, 'lightning': 5, 'rain': 6, 'rainbow': 7, 'rime': 8, 'sandstorm': 9, 'snow': 10}\n{0: 'dew', 1: 'fogsmog', 2: 'frost', 3: 'glaze', 4: 'hail', 5: 'lightning', 6: 'rain', 7: 'rainbow', 8: 'rime', 9: 'sandstorm', 10: 'snow'}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"total = len(full_dataset)\ntrain_size = int(0.7 * total)\nval_size   = int(0.15 * total)\ntest_size  = total - train_size - val_size\n\ntrain_set, val_set, test_set = random_split(\n    full_dataset, \n    [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\nprint(f\"Splits → train: {len(train_set)}, val: {len(val_set)}, test: {len(test_set)}\")\n\ntrain_indices = train_set.indices     \ntrain_labels  = [full_dataset.targets[i] for i in train_indices]\n\nlabel_counts  = Counter(train_labels)\nclass_weights = {cls: 1.0/count for cls, count in label_counts.items()}\nsample_weights = [class_weights[label] for label in train_labels]\n\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),  \n    replacement=True                  \n)\n\nbatch_size = 32\ntrain_loader = DataLoader(\n    train_set,\n    batch_size=batch_size,\n    sampler=sampler,\n    num_workers=2,\n    pin_memory=True\n)\nval_loader = DataLoader(\n    val_set,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\ntest_loader = DataLoader(\n    test_set,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=2\n)\nprint(\"Oversampled train_loader ready, plus val/test loaders\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:38.341979Z","iopub.execute_input":"2025-08-03T09:35:38.342349Z","iopub.status.idle":"2025-08-03T09:35:38.386737Z","shell.execute_reply.started":"2025-08-03T09:35:38.342307Z","shell.execute_reply":"2025-08-03T09:35:38.385715Z"}},"outputs":[{"name":"stdout","text":"Splits → train: 4803, val: 1029, test: 1030\nOversampled train_loader ready, plus val/test loaders\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, patch_size=8, embed_dim=128):\n        super().__init__()\n        self.proj = nn.Conv2d(in_channels=3,\n                              out_channels=embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)              \n        x = x.flatten(2)             \n        x = x.transpose(1, 2)     \n        return x\n\nimages = torch.rand(32, 3, 64, 64)\npatch_embed = PatchEmbedding(8, 128)\noutput = patch_embed(images)\nassert output.shape == torch.Size([32, 64, 128]), \"Something wrong with patch embedding\"\nprint(\"Patch embdding working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:38.388202Z","iopub.execute_input":"2025-08-03T09:35:38.389010Z","iopub.status.idle":"2025-08-03T09:35:38.509142Z","shell.execute_reply.started":"2025-08-03T09:35:38.388982Z","shell.execute_reply":"2025-08-03T09:35:38.508228Z"}},"outputs":[{"name":"stdout","text":"Patch embdding working\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class InputEmbedding(nn.Module):\n    def __init__(self, patch_size, embed_dim, img_size=64):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        num_patches = (img_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n\n    def forward(self, x):\n        x = self.patch_embed(x)  \n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)  \n        x = torch.cat((cls_tokens, x), dim=1)          \n        x = x + self.pos_embedding                    \n        return x\n\n\nimages = torch.rand(32, 3, 64, 64)\ninput_embedding = InputEmbedding(8, 128)\noutput = input_embedding(images)\nassert output.shape == torch.Size([32, 65, 128]), \"Something wrong getting input encoding\"\nprint(\"Input embedding successful\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:38.510371Z","iopub.execute_input":"2025-08-03T09:35:38.510734Z","iopub.status.idle":"2025-08-03T09:35:38.542217Z","shell.execute_reply.started":"2025-08-03T09:35:38.510699Z","shell.execute_reply":"2025-08-03T09:35:38.540994Z"}},"outputs":[{"name":"stdout","text":"Input embedding successful\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class MLPBlock(nn.Module):\n    def __init__(self, embed_dim, mlp_dim, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, mlp_dim)\n        self.fc2 = nn.Linear(mlp_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\nmlp = MLPBlock(embed_dim=128, mlp_dim=512)\nout = mlp(torch.randn(32, 65, 128))  # [B, N+1, D]\nassert out.shape == torch.Size([32, 65, 128]), \"Something wrong in MLP layer\"\nprint(\"MLP block working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:35:38.543241Z","iopub.execute_input":"2025-08-03T09:35:38.543490Z","iopub.status.idle":"2025-08-03T09:35:38.635130Z","shell.execute_reply.started":"2025-08-03T09:35:38.543472Z","shell.execute_reply":"2025-08-03T09:35:38.633973Z"}},"outputs":[{"name":"stdout","text":"MLP block working\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class TransformerEncoderBlock(nn.Module):\n    def __init__(self, mlp_dim, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.mlp = MLPBlock(embed_dim, mlp_dim, dropout=dropout)\n\n    def forward(self, x):\n        residual = x\n        x = self.norm1(x)\n        x = x.transpose(0, 1)\n        x, _ = self.mha(x, x, x)\n        x = x.transpose(0, 1)\n        x = x + residual\n        \n        residual = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = x + residual\n        return x\nblock = TransformerEncoderBlock(mlp_dim=512, embed_dim=128, num_heads=4)\nx = torch.randn(32, 65, 128)  # [B, N+1, D]\nout = block(x)\nassert out.shape == torch.Size([32, 65, 128]), \"Something wrong in Transformer block\"\nprint(\"Transformer block working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:36:10.061027Z","iopub.execute_input":"2025-08-03T09:36:10.061860Z","iopub.status.idle":"2025-08-03T09:36:10.117855Z","shell.execute_reply.started":"2025-08-03T09:36:10.061829Z","shell.execute_reply":"2025-08-03T09:36:10.116771Z"}},"outputs":[{"name":"stdout","text":"Transformer block working\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"dataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\nfig, axs = plt.subplots(6, 6, figsize=(8, 8))\naxs = axs.flatten()\nfor i, ax in enumerate(axs):\n    if i < len(images):  \n        picture = torch.permute(images[i], (1, 2, 0))\n        title = idx_to_class[labels[i].item()]\n        ax.imshow(picture)\n        ax.set_title(title)\n        ax.axis('off')  \n    else:\n        fig.delaxes(ax) \n\nplt.tight_layout()  \nprint(\"Visualising training batch\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:24:16.012160Z","iopub.execute_input":"2025-07-30T17:24:16.012343Z","iopub.status.idle":"2025-07-30T17:24:19.643140Z","shell.execute_reply.started":"2025-07-30T17:24:16.012328Z","shell.execute_reply":"2025-07-30T17:24:19.642252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TinyVGG64_Reg(nn.Module):\n    def __init__(self, num_classes=11):\n        super().__init__()\n        # Block 1: 64 → 32\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.3),               # spatial dropout\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),              # → 32×32\n            nn.Dropout(0.4)\n        )\n        # Block 2: 32 → 16\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.3),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),              # → 16×16\n            nn.Dropout(0.4)\n        )\n        # Global Avg Pool: 16×16 → 1×1\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # Classifier: smaller head\n        self.classifier = nn.Sequential(\n            nn.Flatten(),                    # (B, 64,1,1)→(B,64)\n            nn.Linear(64, 64),               # shrink from 128 to 64\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(64),\n            nn.Dropout(0.5),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.global_avg_pool(x)\n        x = self.classifier(x)\n        return x\n\n# Instantiate & compile\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TinyVGG64_Reg(num_classes=11).to(device)\nmodel = torch.compile(model)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:24:19.644070Z","iopub.execute_input":"2025-07-30T17:24:19.644284Z","iopub.status.idle":"2025-07-30T17:24:21.673672Z","shell.execute_reply.started":"2025-07-30T17:24:19.644257Z","shell.execute_reply":"2025-07-30T17:24:21.672931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:24:21.674618Z","iopub.execute_input":"2025-07-30T17:24:21.675014Z","iopub.status.idle":"2025-07-30T17:24:28.337490Z","shell.execute_reply.started":"2025-07-30T17:24:21.674988Z","shell.execute_reply":"2025-07-30T17:24:28.336909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_EPOCHS=100\nNUM_CLASSES = len(dataset_classes)\ntrain_losses = []\nval_losses = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:24:28.339159Z","iopub.execute_input":"2025-07-30T17:24:28.339458Z","iopub.status.idle":"2025-07-30T17:24:28.343622Z","shell.execute_reply.started":"2025-07-30T17:24:28.339442Z","shell.execute_reply":"2025-07-30T17:24:28.343019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n\n    total_train_loss = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    model.eval()\n    val_loss_sum = 0\n    val_accuracy.reset() \n\n    with torch.no_grad(): # Disable gradient calculation for validation\n        for inputs, targets in val_loader:\n            inputs = inputs.to(device, non_blocking=True)\n            targets = targets.to(device, non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            val_loss_sum += loss.item()\n            val_accuracy.update(outputs, targets) \n\n    avg_val_loss = val_loss_sum / len(val_loader)\n    epoch_val_accuracy = val_accuracy.compute() \n\n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n    if (epoch+1)%10 == 0:\n        print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {epoch_val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:24:28.344347Z","iopub.execute_input":"2025-07-30T17:24:28.344630Z","iopub.status.idle":"2025-07-30T17:29:11.521604Z","shell.execute_reply.started":"2025-07-30T17:24:28.344604Z","shell.execute_reply":"2025-07-30T17:29:11.520447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = list(range(1, NUM_EPOCHS+1))\nplt.plot(epochs, train_losses, 'b', label='Training loss') \nplt.plot(epochs, val_losses, 'r', label='Validation loss') \nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:29:11.522357Z","iopub.status.idle":"2025-07-30T17:29:11.522615Z","shell.execute_reply.started":"2025-07-30T17:29:11.522497Z","shell.execute_reply":"2025-07-30T17:29:11.522508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\ntest_accuracy.reset()\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        test_accuracy.update(outputs, targets) \n\ntotal_test_accuracy = test_accuracy.compute() \ntotal_test_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:29:11.523503Z","iopub.status.idle":"2025-07-30T17:29:11.523741Z","shell.execute_reply.started":"2025-07-30T17:29:11.523637Z","shell.execute_reply":"2025-07-30T17:29:11.523647Z"}},"outputs":[],"execution_count":null}]}